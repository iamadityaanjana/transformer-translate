{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d0f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import torchmetrics\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6969b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"datasource\": 'opus_books',\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"it\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "    }\n",
    "\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
    "    return str(Path('.') / model_folder / model_filename)\n",
    "\n",
    "# Find the latest weights file in the weights folder\n",
    "def latest_weights_file_path(config):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}*\"\n",
    "    weights_files = list(Path(model_folder).glob(model_filename))\n",
    "    if len(weights_files) == 0:\n",
    "        return None\n",
    "    weights_files.sort()\n",
    "    return str(weights_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df21920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class BilingualDataset(Dataset):\n",
    "    def __init__(self , ds, tokenizer_src , tokenizer_tgt, src_lang , tgt_lang , seq_len):\n",
    "\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token_id = tokenizer_tgt.token_to_id(\"[PAD]\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence length exceeds maximum sequence length\")\n",
    "\n",
    "        encoder_input = torch.cat([self.sos_token,\n",
    "                                   torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                                   self.eos_token,\n",
    "                                   torch.tensor([self.pad_token_id] * enc_num_padding_tokens, dtype=torch.int64)\n",
    "                                  ])\n",
    "        \n",
    "        decoder_input = torch.cat([self.sos_token,\n",
    "                                   torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                                   torch.tensor([self.pad_token_id] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "                                  ])\n",
    "        \n",
    "        label = torch.cat([torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                           self.eos_token,\n",
    "                           torch.tensor([self.pad_token_id] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "                          ])\n",
    "        \n",
    "        assert encoder_input.shape[0] == self.seq_len\n",
    "        assert decoder_input.shape[0] == self.seq_len\n",
    "        assert label.shape[0] == self.seq_len\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,\n",
    "            \"decoder_input\": decoder_input,\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token_id).unsqueeze(0).unsqueeze(0).int(),\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token_id).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
    "            \"label\": label,\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text\n",
    "        }\n",
    "    \n",
    "\n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc493904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class InputEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self , d_model:int , vocab_size:int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size , d_model)\n",
    "\n",
    "    def forward(self , x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self , d_model:int , seq_len:int , dropout:float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #matrix of shape (seq_len,d_model)\n",
    "        #Positional encoding\n",
    "        pe = torch.zeros(seq_len,d_model) \n",
    "\n",
    "        #create a vector of shape (seq_len)\n",
    "        position = torch.arange(0 , seq_len , dtype=torch.float).unsqueeze(1) #(seq_len ,1)\n",
    "        div_term = torch.exp(torch.arange(0,d_model , 2).float() * (-math.log(10000.0)/d_model))\n",
    "\n",
    "        #apply sine to even position and cosine to odd\n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        pe[:,1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0) #(1, seq_len , d_model)\n",
    "\n",
    "        self.register_buffer('pe',pe)\n",
    "\n",
    "    def forward(self ,x):\n",
    "        x = x+ (self.pe[: , :x.shape[1], :].requires_grad_(False))\n",
    "        return self.dropout(x)\n",
    "\n",
    "    \n",
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self , eps:float = 10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) #beta\n",
    "\n",
    "    def forward(self , x):\n",
    "        mean = x.mean(dim=-1 ,keepdim = True)\n",
    "        std = x.std(dim = -1 , keepdim= True)\n",
    "        return self.alpha * (x-mean) / (self.eps + std) + self.bias\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self , d_model:int , d_ff:int , dropout:float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model , d_ff) #W1 and B1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff , d_model) #W2 and B2\n",
    "        \n",
    "\n",
    "    def forward(self , x):\n",
    "        #(Batch, Seq_len , d_model) --> (Batch , Seq_len , d_ff) --> (Batch , Seq_len , d_model)\n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self , d_model:int , num_heads:int , dropout:float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = num_heads\n",
    "        assert d_model % num_heads == 0 , \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.w_q = nn.Linear(d_model , d_model)\n",
    "        self.w_k = nn.Linear(d_model , d_model)\n",
    "        self.w_v = nn.Linear(d_model , d_model)\n",
    "        self.w_o = nn.Linear(d_model , d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query , key , value , mask , dropout:nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "\n",
    "\n",
    "        # (Batch , h , Seq_len , d_k) @ (Batch , h , d_k , Seq_len) --> (Batch , h , Seq_len , Seq_len)\n",
    "        attention_score = (query @ key.transpose(-2 , -1)) / math.sqrt(d_k) #(B , h , Seq_len , Seq_len)\n",
    "        if mask is not None:\n",
    "            attention_score = attention_score.masked_fill(mask == 0 , -1e9) ##replace with a very small value\n",
    "        attention_score = attention_score.softmax(dim = -1) #(B , h , Seq_len , Seq_len)\n",
    "\n",
    "        if dropout is not None:\n",
    "            attention_score = dropout(attention_score)\n",
    "\n",
    "        return (attention_score @ value) , attention_score #(B , h , Seq_len , d_k)\n",
    "\n",
    "    def forward(self , query , key , value , mask):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        #linear projections\n",
    "        Q = self.w_q(query) #(B , Seq_len , d_model)\n",
    "        K = self.w_k(key)   #(B , Seq_len , d_model)\n",
    "        V = self.w_v(value) #(B , Seq_len , d_model)\n",
    "\n",
    "        #split into h heads\n",
    "        Q = Q.view(Q.shape[0] , Q.shape[1] , self.h , self.d_k).transpose(1,2) #(B , h , Seq_len , d_k)\n",
    "        K = K.view(K.shape[0] , K.shape[1] , self.h , self.d_k).transpose(1,2) #(B , h , Seq_len , d_k)\n",
    "        V = V.view(V.shape[0] , V.shape[1] , self.h , self.d_k).transpose(1,2) #(B , h , Seq_len , d_k)\n",
    "\n",
    "        x, self.attention_score = MultiHeadAttention.attention(Q , K , V , mask , self.dropout) #(B , h , Seq_len , d_k)\n",
    "\n",
    "\n",
    "        # (batch , h , seq_len , d_k) --> (batch , seq_len , h , d_k) --> (batch , seq_len , d_model)\n",
    "        x = x.transpose(1,2).contiguous().view(x.shape[0] , -1 , self.h * self.d_k) #(B , Seq_len , d_model)\n",
    "\n",
    "        return self.w_o(x) #(B , Seq_len , d_model)\n",
    "\n",
    "    \n",
    "class SublayerConnection(nn.Module):\n",
    "\n",
    "    def __init__(self , dropout:float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self , x , sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self , self_attention_block: MultiHeadAttention, feed_forward_block: FeedForward , dropout:float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([SublayerConnection(dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self , x , src_mask):\n",
    "        x = self.residual_connections[0](x , lambda x: self.self_attention_block(x , x , x , src_mask))\n",
    "        x = self.residual_connections[1](x , self.feed_forward_block)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self , layers:nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self , x , src_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x , src_mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self , self_attention_block: MultiHeadAttention , cross_attention_block: MultiHeadAttention , feed_forward_block: FeedForward , dropout:float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([SublayerConnection(dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self , x , encoder_output, src_mask , tgt_mask):\n",
    "        x = self.residual_connections[0](x , lambda x: self.self_attention_block(x , x , x , tgt_mask))\n",
    "        x = self.residual_connections[1](x , lambda x: self.cross_attention_block(x , encoder_output , encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x , self.feed_forward_block)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self , layers:nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self , x , encoder_output , src_mask , tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x , encoder_output , src_mask , tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self , d_model:int , vocab_size:int) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model , vocab_size)\n",
    "\n",
    "    def forward(self , x):\n",
    "        #(batch , seq_len , d_model) --> (batch , seq_len , vocab_size)\n",
    "        return torch.log_softmax(self.proj(x) , dim = -1)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self , encoder:Encoder , decoder:Decoder , src_embed:nn.Module , tgt_embed:nn.Module ,src_pos:PositionalEncoding , tgt_pos:PositionalEncoding ,projection_layer:ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection = projection_layer\n",
    "\n",
    "    def encode(self , src , src_mask):\n",
    "        return self.encoder(self.src_pos(self.src_embed(src)) , src_mask)\n",
    "   \n",
    "    def decode(self , tgt , encoder_output , src_mask , tgt_mask):\n",
    "        return self.decoder(self.tgt_pos(self.tgt_embed(tgt)) , encoder_output , src_mask , tgt_mask)\n",
    "\n",
    "    def project(self , x):\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "def build_transformer(src_vocab_size:int , tgt_vocab_size:int , src_seq_len:int , tgt_seq_len:int , d_model:int = 512 , N:int = 6 , H:int = 8 , dropout:float = 0.1 , d_ff:int = 2048) -> Transformer:\n",
    "    #Embedding layers\n",
    "    src_embed = InputEmbedding(d_model , src_vocab_size)\n",
    "    tgt_embed = InputEmbedding(d_model , tgt_vocab_size)\n",
    "\n",
    "    #Positional Encoding layers\n",
    "    src_pos = PositionalEncoding(d_model , src_seq_len , dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model , tgt_seq_len , dropout)\n",
    "\n",
    "    #Encoder layers\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttention(d_model , H , dropout)\n",
    "        feed_forward_block = FeedForward(d_model , d_ff , dropout)\n",
    "        encoder_layer = EncoderLayer(encoder_self_attention_block , feed_forward_block , dropout)\n",
    "        encoder_blocks.append(encoder_layer)\n",
    "\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttention(d_model , H , dropout)\n",
    "        cross_attention_block = MultiHeadAttention(d_model , H , dropout)\n",
    "        feed_forward_block = FeedForward(d_model , d_ff , dropout)\n",
    "        decoder_layer = DecoderBlock(decoder_self_attention_block , cross_attention_block , feed_forward_block , dropout)\n",
    "        decoder_blocks.append(decoder_layer)\n",
    "\n",
    "    #create the encoder and decoder\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    #create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model , tgt_vocab_size)\n",
    "\n",
    "    #create the transformer model\n",
    "    model = Transformer(encoder , decoder , src_embed , tgt_embed , src_pos , tgt_pos , projection_layer)\n",
    "\n",
    "    #initialize the parameters\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5305c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]\n",
    "\n",
    "\n",
    "def get_or_build_tokenizer(config , ds , lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Build tokenizer\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(decoder_input, encoder_output, source_mask, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)\n",
    "\n",
    "\n",
    "def get_ds(config):\n",
    "    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split='train')\n",
    "\n",
    "\n",
    "    #Build tokenizers\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "\n",
    "    #train and val split\n",
    "    train_ds_size = int(len(ds_raw) * 0.9)\n",
    "    ds_train = ds_raw.select(range(0, train_ds_size))\n",
    "    ds_val = ds_raw.select(range(train_ds_size, len(ds_raw)))\n",
    "\n",
    "    train_ds = BilingualDataset(ds_train, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(ds_val, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "    \n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
    "\n",
    "\n",
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'])\n",
    "    return model\n",
    "\n",
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(\n",
    "                0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "            \n",
    "            # Print the source, target and model output\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "    \n",
    "    if writer:\n",
    "        # Evaluate the character error rate\n",
    "        # Compute the char error rate \n",
    "        metric = torchmetrics.CharErrorRate()\n",
    "        cer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation cer', cer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the word error rate\n",
    "        metric = torchmetrics.WordErrorRate()\n",
    "        wer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation wer', wer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the BLEU metric\n",
    "        metric = torchmetrics.BLEUScore()\n",
    "        bleu = metric(predicted, expected)\n",
    "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "\n",
    "\n",
    "def train_model(config):\n",
    "    # Define the device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "    if (device == 'cuda'):\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "    elif (device == 'mps'):\n",
    "        print(f\"Device name: <mps>\")\n",
    "    else:\n",
    "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
    "        print(\"      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\")\n",
    "        print(\"      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\")\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # Make sure the weights folder exists\n",
    "    Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "\n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "    if model_filename:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_tgt.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(decoder_input, encoder_output, encoder_mask, decoder_mask) # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device) # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Run validation at the end of every epoch\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d5893",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "config = get_config()\n",
    "config['batch_size'] = 16\n",
    "config['preload'] = None\n",
    "config['num_epochs'] = 10\n",
    "\n",
    "train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e5dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence: str):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    config = get_config()\n",
    "    tokenizer_src = Tokenizer.from_file(str(Path(config['tokenizer_file'].format(config['lang_src']))))\n",
    "    tokenizer_tgt = Tokenizer.from_file(str(Path(config['tokenizer_file'].format(config['lang_tgt']))))\n",
    "    \n",
    "    model = build_transformer(tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size(), config[\"seq_len\"], config['seq_len'], d_model=config['d_model']).to(device)\n",
    "    \n",
    "    # Load the latest weights\n",
    "    model_filename = latest_weights_file_path(config)\n",
    "    state = torch.load(model_filename)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        source = tokenizer_src.encode(sentence)\n",
    "        source = torch.cat([\n",
    "            torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64), \n",
    "            torch.tensor(source.ids, dtype=torch.int64),\n",
    "            torch.tensor([tokenizer_src.token_to_id('[EOS]')], dtype=torch.int64),\n",
    "            torch.tensor([tokenizer_src.token_to_id('[PAD]')] * (config['seq_len'] - len(source.ids) - 2), dtype=torch.int64)\n",
    "        ], dim=0).to(device)\n",
    "        \n",
    "        source_mask = (source != tokenizer_src.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int().to(device)\n",
    "        \n",
    "        encoder_output = model.encode(source, source_mask)\n",
    "        \n",
    "        decoder_input = torch.empty(1, 1).fill_(tokenizer_tgt.token_to_id('[SOS]')).type_as(source).to(device)\n",
    "        \n",
    "        print(f\"Translating: {sentence}\")\n",
    "        \n",
    "        while True:\n",
    "            if decoder_input.size(1) == config['seq_len']:\n",
    "                break\n",
    "            \n",
    "            decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "            \n",
    "            out = model.decode(decoder_input, encoder_output, source_mask, decoder_mask)\n",
    "            \n",
    "            prob = model.project(out[:, -1])\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            \n",
    "            decoder_input = torch.cat(\n",
    "                [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "            )\n",
    "            \n",
    "            if next_word == tokenizer_tgt.token_to_id('[EOS]'):\n",
    "                break\n",
    "        \n",
    "        translation = tokenizer_tgt.decode(decoder_input.squeeze(0).detach().cpu().numpy())\n",
    "        print(f\"Translation: {translation}\")\n",
    "\n",
    "# Test with an English sentence\n",
    "translate(\"I am learning to code.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
